{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "第4章_章末演習問題.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WehYXzWpwhi1"
      },
      "source": [
        "第4章の章末演習問題"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRoPRKcMwlbZ"
      },
      "source": [
        "### [1] 携帯電話やデジタルカメラで、赤、青、緑の物体の写真を何枚か撮りましょう（カメラがない場合は、インターネットからダウンロードすることもできます）。\n",
        "※ ここではGoogle Colaraboratoryでの実行を想定しています。\n"
      ]
    },
    {
      "source": [
        "※本フォルダに、無料写真素材　写真AC（商用利用可）のデータを配置しています。\n",
        "\n",
        "apple.jpg、sky.jpg、forest.jpg\n",
        "\n",
        "\n",
        "https://www.photo-ac.com/main/detail/345453?title=%E3%83%AA%E3%83%B3%E3%82%B4&searchId=68658080\n",
        "\n",
        "https://www.photo-ac.com/main/detail/3343123?title=%E3%81%99%E3%81%A3%E3%81%8D%E3%82%8A%E6%99%B4%E3%82%8C%E3%82%84%E3%81%8B%E3%81%AA%E5%A4%8F%E3%81%AE%E7%A9%BA&searchId=68660407\n",
        "\n",
        "https://www.photo-ac.com/main/detail/3518379?title=%E6%A8%B9%E6%9C%A8_%E6%A3%AE%E6%9E%97_3&searchId=68662146\n",
        "\n"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YnFAcuysEQDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#### （a）各画像を読み込み、テンソルに変換してください。"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FSZC8BAJw9A-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kduB0LXvxL0b"
      },
      "source": [
        "# 回答\n",
        "import numpy as np\n",
        "import torch\n",
        "torch.set_printoptions(edgeitems=2, threshold=50)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imageio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2336, 4160, 3), (4160, 2336, 3), (3120, 4160, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "img_arr_r = imageio.imread('../data/p1ch4/red.jpg')\n",
        "img_arr_g = imageio.imread('../data/p1ch4/green.jpg')\n",
        "img_arr_b = imageio.imread('../data/p1ch4/blue.jpg')\n",
        "img_arr_r.shape, img_arr_g.shape, img_arr_b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "from_numpy() takes no keyword arguments",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-11-7a1a525f72d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimg_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_arr_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mimg_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_arr_g\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mimg_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_arr_b\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mout_r\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_r\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mout_g\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_g\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mTypeError\u001b[0m: from_numpy() takes no keyword arguments"
          ]
        }
      ],
      "source": [
        "img_r = torch.from_numpy(img_arr_r)\n",
        "img_g = torch.from_numpy(img_arr_g)\n",
        "img_b = torch.from_numpy(img_arr_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "img_r_f = img_r.to(dtype=torch.float)\n",
        "img_g_f = img_g.to(dtype=torch.float)\n",
        "img_b_f = img_b.to(dtype=torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2336, 4160]),\n",
              " torch.Size([3, 4160, 2336]),\n",
              " torch.Size([3, 3120, 4160]))"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "out_r = img_r_f.permute(2, 0, 1)\n",
        "out_g = img_g_f.permute(2, 0, 1)\n",
        "out_b = img_b_f.permute(2, 0, 1)\n",
        "out_r.shape, out_g.shape, out_b.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "out_r.dtype"
      ]
    },
    {
      "source": [
        "#### (b）各画像テンソルについて、.mean()メソッドを使用して、画像の明るさを求めてください。"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "YO24ioODyKtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_7dlCScyWYr"
      },
      "source": [
        "# 回答 dim=1で次元1で平均する\n",
        "torch.mean(out_r, dim=1), torch.mean(out_g, dim=1), torch.mean(out_b, dim=1)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 79.6667,  78.6667,  ...,  76.0000,  76.0000],\n",
              "         [ 76.6667,  75.6667,  ...,  74.0000,  76.0000],\n",
              "         ...,\n",
              "         [186.0000, 185.0000,  ...,  42.0000,  41.0000],\n",
              "         [184.0000, 184.0000,  ...,  44.0000,  43.0000]]),\n",
              " tensor([[124.0000, 125.0000,  ..., 137.3333, 133.3333],\n",
              "         [127.0000, 126.0000,  ..., 131.3333, 140.3333],\n",
              "         ...,\n",
              "         [ 92.0000,  93.0000,  ..., 150.0000, 155.0000],\n",
              "         [ 94.0000,  88.0000,  ..., 147.0000, 149.3333]]),\n",
              " tensor([[145.3333, 145.3333,  ..., 113.0000, 113.0000],\n",
              "         [141.3333, 143.3333,  ..., 112.0000, 113.0000],\n",
              "         ...,\n",
              "         [173.6667, 169.6667,  ..., 210.6667, 211.6667],\n",
              "         [170.6667, 167.6667,  ..., 211.6667, 212.6667]]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([3, 2336, 4160]), torch.Size([3, 4160]))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# 次元1が平均されたことで次元が減ったことの確認\n",
        "out_r.shape, torch.mean(out_r, dim=1).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6rfvnbqJloe"
      },
      "source": [
        "#### （c）各画像の各チャンネルの平均を取ってください。求めたチャンネルの平均値だけから、赤、緑、青の物体を識別できるか確認してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41982_5lCCUL"
      },
      "source": [
        "# さらに、次元1，2で平均する　平均対象の次元を逐次的に指定する\n",
        "# 回答 赤\n",
        "torch.mean(out_r, dim=1).mean(dim=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([132.3350, 109.1540, 109.0145]),)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([143.6180, 157.2123, 145.9531])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# 回答 緑\n",
        "torch.mean(out_g, dim=1).mean(dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([125.8333, 124.3068, 106.6931])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# 回答 青\n",
        "torch.mean(out_b, dim=1).mean(dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdWttSfHzGKz"
      },
      "source": [
        "### [2] Pythonのソースコードを含む比較的大きなファイルを用意してください。\n",
        "\n",
        "※https://github.com/YutaroOgawa/pytorch_advanced/blob/master/1_image_classification/utils/dataloader_image_classification.py\n",
        "\n",
        "をフォルダ内に用意しています。\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yzctZuLzODR"
      },
      "source": [
        "#### （a）ソースファイル内のすべての単語のインデックスを作成してください (トークン化はシンプルにしても複雑にしても構いません。 最初は正規表現r\"[^a-zA-Z0-9_]+\"とスペースで、単語を一度置き換えることをおすすめします)。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFq3TaQBy-ZE"
      },
      "source": [
        "# 回答\n",
        "with open('./data_loader.py', encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['import', 'glob', 'import', 'os', 'path', 'as', 'osp', 'import', 'torch', 'utils', 'data', 'as', 'data', 'from', 'torchvision', 'import', 'models', 'transforms', 'from', 'PIL', 'import', 'Image', 'class', 'ImageTransform', 'RandomResizedCrop', 'RandomHorizontalFlip', 'Attributes', 'resize', 'int', 'mean', 'R', 'G', 'B', 'std', 'R', 'G', 'B', 'def', '__init__', 'self', 'resize', 'mean', 'std', 'self', 'data_transform', 'train', 'transforms', 'Compose', 'transforms', 'RandomResizedCrop', 'resize', 'scale', '0', '5', '1', '0', 'transforms', 'RandomHorizontalFlip', 'transforms', 'ToTensor', 'transforms', 'Normalize', 'mean', 'std', 'val', 'transforms', 'Compose', 'transforms', 'Resize', 'resize', 'transforms', 'CenterCrop', 'resize', 'resize', 'resize', 'transforms', 'ToTensor', 'transforms', 'Normalize', 'mean', 'std', 'def', '__call__', 'self', 'img', 'phase', 'train', 'Parameters', 'phase', 'train', 'or', 'val', 'return', 'self', 'data_transform', 'phase', 'img', 'def', 'make_datapath_list', 'phase', 'train', 'Parameters', 'phase', 'train', 'or', 'val', 'Returns', 'path_list', 'list', 'rootpath', 'data', 'hymenoptera_data', 'target_path', 'osp', 'join', 'rootpath', 'phase', 'jpg', 'print', 'target_path', 'path_list', 'glob', 'for', 'path', 'in', 'glob', 'glob', 'target_path', 'path_list', 'append', 'path', 'return', 'path_list', 'class', 'HymenopteraDataset', 'data', 'Dataset', 'Dataset', 'PyTorch', 'Dataset', 'Attributes', 'file_list', 'transform', 'object', 'phase', 'train', 'or', 'test', 'def', '__init__', 'self', 'file_list', 'transform', 'None', 'phase', 'train', 'self', 'file_list', 'file_list', 'self', 'transform', 'transform', 'self', 'phase', 'phase', 'train', 'or', 'val', 'def', '__len__', 'self', 'return', 'len', 'self', 'file_list', 'def', '__getitem__', 'self', 'index', 'Tensor', 'index', 'img_path', 'self', 'file_list', 'index', 'img', 'Image', 'open', 'img_path', 'RGB', 'img_transformed', 'self', 'transform', 'img', 'self', 'phase', 'torch', 'Size', '3', '224', '224', 'if', 'self', 'phase', 'train', 'label', 'img_path', '30', '34', 'elif', 'self', 'phase', 'val', 'label', 'img_path', '28', '32', 'if', 'label', 'ants', 'label', '0', 'elif', 'label', 'bees', 'label', '1', 'return', 'img_transformed', 'label']\n"
          ]
        }
      ],
      "source": [
        "# 正規表現で抽出 以下のURLを参考\n",
        "# https://techacademy.jp/magazine/19307\n",
        "import re\n",
        "words_in_file = re.findall(r\"[a-zA-Z0-9_]+\", text) # 文字と数字\n",
        "\n",
        "print(words_in_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBEZN-2gzy-P"
      },
      "source": [
        "#### （b）本章の「高慢と偏見」で作ったインデックスと比較してみてください。どちらの方がサイズは大きいですか？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e-l9C2S5Ihj"
      },
      "source": [
        "# 回答\n",
        "len(words_in_file)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "230"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# 重複を消し、並び替える\n",
        "word_list = list(set(words_in_file)) # setで同一語を1つにする= 重複排除\n",
        "word_list = sorted(word_list)\n",
        "len(word_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(90, 86)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "word2index_dict = {word: i for (i, word) in enumerate(word_list)}\n",
        "\n",
        "len(word2index_dict), word2index_dict['transform']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ei2PjV2b1pVy"
      },
      "source": [
        "#### （c）ソースコードファイルのワンホットエンコーディングを作成してください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ij9x9461uDC"
      },
      "source": [
        "# 回答\n",
        "word_t = torch.zeros(len(words_in_file), len(word2index_dict))\n",
        "for i , word in enumerate(words_in_file):\n",
        "    word_index = word2index_dict[word]\n",
        "    word_t[i][word_index] = 1 # i番目の単語の辞書word_indexの列に1を入れている\n",
        "    print('{:2} {:4} {}'.format(i, word_index, word))\n",
        "    \n",
        "print(word_t.shape)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0   54 import\n 1   48 glob\n 2   54 import\n 3   69 os\n 4   71 path\n 5   38 as\n 6   70 osp\n 7   54 import\n 8   83 torch\n 9   88 utils\n10   41 data\n11   38 as\n12   41 data\n13   47 from\n14   84 torchvision\n15   54 import\n16   65 models\n17   87 transforms\n18   47 from\n19   20 PIL\n20   54 import\n21   16 Image\n22   40 class\n23   17 ImageTransform\n24   26 RandomResizedCrop\n25   25 RandomHorizontalFlip\n26    9 Attributes\n27   75 resize\n28   57 int\n29   64 mean\n30   23 R\n31   14 G\n32   10 B\n33   80 std\n34   23 R\n35   14 G\n36   10 B\n37   43 def\n38   34 __init__\n39   79 self\n40   75 resize\n41   64 mean\n42   80 std\n43   79 self\n44   42 data_transform\n45   85 train\n46   87 transforms\n47   12 Compose\n48   87 transforms\n49   26 RandomResizedCrop\n50   75 resize\n51   78 scale\n52    0 0\n53    8 5\n54    1 1\n55    0 0\n56   87 transforms\n57   25 RandomHorizontalFlip\n58   87 transforms\n59   31 ToTensor\n60   87 transforms\n61   19 Normalize\n62   64 mean\n63   80 std\n64   89 val\n65   87 transforms\n66   12 Compose\n67   87 transforms\n68   27 Resize\n69   75 resize\n70   87 transforms\n71   11 CenterCrop\n72   75 resize\n73   75 resize\n74   75 resize\n75   87 transforms\n76   31 ToTensor\n77   87 transforms\n78   19 Normalize\n79   64 mean\n80   80 std\n81   43 def\n82   32 __call__\n83   79 self\n84   51 img\n85   73 phase\n86   85 train\n87   21 Parameters\n88   73 phase\n89   85 train\n90   68 or\n91   89 val\n92   76 return\n93   79 self\n94   42 data_transform\n95   73 phase\n96   51 img\n97   43 def\n98   63 make_datapath_list\n99   73 phase\n100   85 train\n101   21 Parameters\n102   73 phase\n103   85 train\n104   68 or\n105   89 val\n106   28 Returns\n107   72 path_list\n108   62 list\n109   77 rootpath\n110   41 data\n111   49 hymenoptera_data\n112   81 target_path\n113   70 osp\n114   58 join\n115   77 rootpath\n116   73 phase\n117   59 jpg\n118   74 print\n119   81 target_path\n120   72 path_list\n121   48 glob\n122   46 for\n123   71 path\n124   55 in\n125   48 glob\n126   48 glob\n127   81 target_path\n128   72 path_list\n129   37 append\n130   71 path\n131   76 return\n132   72 path_list\n133   40 class\n134   15 HymenopteraDataset\n135   41 data\n136   13 Dataset\n137   13 Dataset\n138   22 PyTorch\n139   13 Dataset\n140    9 Attributes\n141   45 file_list\n142   86 transform\n143   66 object\n144   73 phase\n145   85 train\n146   68 or\n147   82 test\n148   43 def\n149   34 __init__\n150   79 self\n151   45 file_list\n152   86 transform\n153   18 None\n154   73 phase\n155   85 train\n156   79 self\n157   45 file_list\n158   45 file_list\n159   79 self\n160   86 transform\n161   86 transform\n162   79 self\n163   73 phase\n164   73 phase\n165   85 train\n166   68 or\n167   89 val\n168   43 def\n169   35 __len__\n170   79 self\n171   76 return\n172   61 len\n173   79 self\n174   45 file_list\n175   43 def\n176   33 __getitem__\n177   79 self\n178   56 index\n179   30 Tensor\n180   56 index\n181   52 img_path\n182   79 self\n183   45 file_list\n184   56 index\n185   51 img\n186   16 Image\n187   67 open\n188   52 img_path\n189   24 RGB\n190   53 img_transformed\n191   79 self\n192   86 transform\n193   51 img\n194   79 self\n195   73 phase\n196   83 torch\n197   29 Size\n198    4 3\n199    2 224\n200    2 224\n201   50 if\n202   79 self\n203   73 phase\n204   85 train\n205   60 label\n206   52 img_path\n207    5 30\n208    7 34\n209   44 elif\n210   79 self\n211   73 phase\n212   89 val\n213   60 label\n214   52 img_path\n215    3 28\n216    6 32\n217   50 if\n218   60 label\n219   36 ants\n220   60 label\n221    0 0\n222   44 elif\n223   60 label\n224   39 bees\n225   60 label\n226    1 1\n227   76 return\n228   53 img_transformed\n229   60 label\ntorch.Size([230, 90])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0.,  ..., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "word_t.sum(dim=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT84rW88KnwC"
      },
      "source": [
        "#### （d）今回のエンコーディングで失われる情報は何でしょうか？本章での「高慢と偏見」のエンコーディングで失われた情報と比較してみてください。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kgJ_bDJzKrcA"
      },
      "source": [
        "# 回答\n",
        "# 単語の並び、単語間の関係、回数など"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xqu60yKps9Rh"
      },
      "source": [
        "#　省略"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RKZsWxPz6kz"
      },
      "source": [
        "以上。\n"
      ]
    }
  ]
}